import json
import multiprocessing as mp
import os
import shutil
import time
from collections import defaultdict
from logging import Logger
from typing import Dict, List
from urllib.parse import urljoin

import thirdai
from platform_common.file_handler import expand_cloud_buckets_and_directories
from platform_common.logging.logcodes import LogCode
from platform_common.ndb.ndbv2_parser import parse_doc
from platform_common.ndb.utils import delete_docs_and_remove_files
from platform_common.pydantic_models.feedback_logs import ActionType, FeedbackLog
from platform_common.pydantic_models.llm_config import LLMProvider
from platform_common.pydantic_models.training import FileInfo, FileLocation, TrainConfig
from thirdai import neural_db_v2 as ndbv2
from thirdai.neural_db_v2.chunk_stores import PandasChunkStore
from thirdai.neural_db_v2.retrievers import FinetunableRetriever
from train_job.llm.api_clients import llm_classes
from train_job.models.model import Model
from train_job.reporter import Reporter
from train_job.utils import check_disk, get_directory_size


class NeuralDBV2(Model):
    def __init__(self, config: TrainConfig, reporter: Reporter, logger: Logger):
        super().__init__(config=config, reporter=reporter, logger=logger)

        if self.config.base_model_id:
            base_model_path = os.path.join(
                self.config.model_bazaar_dir,
                "models",
                self.config.base_model_id,
                "model",
                "model.ndb",
            )
            self.logger.info(
                f"Starting training from base model: {base_model_path}",
                code=LogCode.MODEL_INIT,
            )
            # It seems like this can cause an issue if it runs at the same time as
            # a deployment job starts because the DB files are modified by the deployment
            # job which can cause errors during copying.
            shutil.copytree(
                base_model_path,
                self.ndb_save_path(),
                ignore=shutil.ignore_patterns("*.tmpdb"),
                dirs_exist_ok=True,
            )
            self.db = ndbv2.NeuralDB.load(self.ndb_save_path())

            with open(ndbv2.NeuralDB.metadata_path(self.ndb_save_path()), "r") as f:
                ndb_save_metadata = json.load(f)
            chunk_store_name = ndb_save_metadata["chunk_store_name"]
            self.on_disk = chunk_store_name != "PandasChunkStore"
        else:
            ndb_options = self.config.model_options
            splade = ndb_options.advanced_search

            self.on_disk = ndb_options.on_disk

            self.logger.info(
                f"NDB options - advanced_search: {splade}, on_disk: {ndb_options.on_disk}"
            )

            self.logger.info("Creating new NDBv2 model", code=LogCode.MODEL_INIT)
            if ndb_options.on_disk:
                self.db = ndbv2.NeuralDB(save_path=self.ndb_save_path(), splade=splade)
            else:
                # For the in memory model we create the chunk store in memory
                # but the retriever is still on disk. The reason for this is
                # because it's a good tradeoff between construction/inference time
                # and RAM. We'll get most of the speed gains but at around half
                # the RAM usage per model. We could have two flags for
                # on_disk_chunk_store and on_disk_retriever but that would conflict
                # with the existing on disk flag and expose confusing internals to users.
                self.db = ndbv2.NeuralDB(
                    chunk_store=PandasChunkStore(),
                    retriever=FinetunableRetriever(self.retriever_save_path()),
                    splade=splade,
                )

        if config.generative_supervision:
            self.llm_response_dir = self.model_dir / config.llm_config.provider.value
            self.llm_response_dir.mkdir(parents=True, exist_ok=True)

            llm_args = {
                "track_usage_at": str(self.llm_response_dir / "usage.json"),
            }

            if config.llm_config.provider != LLMProvider.onprem:
                llm_args["api_key"] = self.config.llm_config.api_key
                if self.config.llm_config.base_url:
                    llm_args["base_url"] = self.config.llm_config.base_url

                if self.config.llm_config.model_name:
                    llm_args["model_name"] = self.config.llm_config.model_name
            else:
                llm_args["base_url"] = urljoin(
                    self.config.model_bazaar_endpoint, "/on-prem-llm/v1"
                )
                llm_args["api_key"] = "sk-no-key-required"
                llm_args["model_name"] = "onPrem"

            self.llm = llm_classes.get(self.config.llm_config.provider)(**llm_args)

    def retriever_save_path(self):
        return os.path.join(self.model_dir, "train_retriever")

    def ndb_save_path(self):
        return os.path.join(self.model_dir, "model", "model.ndb")

    def doc_save_path(self):
        return os.path.join(self.ndb_save_path(), "documents")

    def unsupervised_files(self) -> List[FileInfo]:
        return expand_cloud_buckets_and_directories(self.config.data.unsupervised_files)

    def supervised_files(self) -> List[FileInfo]:
        all_files = expand_cloud_buckets_and_directories(
            self.config.data.supervised_files
        )
        for file in all_files:
            if file.ext() != ".csv" and file.ext() != ".jsonl":
                raise ValueError(
                    f"Only CSV or jsonl files are supported for NDB supervised training. Found file {file.path}"
                )
        return all_files

    def unsupervised_train(self, files: List[FileInfo], batch_size=500):
        self.logger.debug("Starting unsupervised training.")

        n_jobs = max(1, min(os.cpu_count() - 6, 20))

        self.logger.debug(f"Using {n_jobs} parsing jobs")

        doc_save_dir = self.doc_save_path()
        tmp_dir = self.data_dir / "unsupervised"

        os.makedirs(tmp_dir, exist_ok=True)

        docs_indexed = 0
        successfully_indexed_files = 0

        batches = [files[i : i + batch_size] for i in range(0, len(files), batch_size)]
        with mp.Pool(processes=n_jobs) as pool:
            first_batch_start = time.perf_counter()
            curr_batch = pool.starmap(
                parse_doc,
                [(doc, doc_save_dir, tmp_dir) for doc in batches[0]],
                chunksize=10,
            )
            first_batch_end = time.perf_counter()
            self.logger.debug(
                f"First batch parsed in {first_batch_end - first_batch_start:.3f}s"
            )

            for i in range(len(batches)):
                start = time.perf_counter()
                if i + 1 < len(batches):
                    next_batch = pool.starmap_async(
                        parse_doc,
                        [(doc, doc_save_dir, tmp_dir) for doc in batches[i + 1]],
                        chunksize=10,
                    )
                else:
                    next_batch = None

                docs = []
                for doc_idx, doc in enumerate(curr_batch):
                    if not doc:
                        msg = f"Unable to parse {batches[i][doc_idx].path}. Unsupported filetype."
                        self.logger.error(msg, code=LogCode.MODEL_INSERT)
                        self.reporter.report_warning(
                            model_id=self.config.model_id,
                            message=msg,
                        )
                    else:
                        docs.append(doc)

                index_start = time.perf_counter()
                self.db.insert(docs)
                index_end = time.perf_counter()

                docs_indexed += len(curr_batch)
                successfully_indexed_files += len(docs)

                if next_batch:
                    next_batch.wait()
                    curr_batch = next_batch.get()

                end = time.perf_counter()
                self.logger.debug(
                    f"Batch {i+1} inserted in {end - start:.3f}s, insertion time: {index_end - index_start:.3f}s, "
                    f"total documents indexed so far: {docs_indexed}"
                )

        total_chunks = self.db.retriever.retriever.size()
        self.logger.info(
            f"Completed unsupervised training total_docs={docs_indexed} total_chunks={total_chunks}.",
            code=LogCode.MODEL_INSERT,
        )

        upsert_doc_ids = [
            file.source_id
            for file in files
            if file.source_id and file.options.get("upsert", False)
        ]

        self.logger.info(
            f"Found {len(upsert_doc_ids)} docs to upsert, removing old versions",
            code=LogCode.MODEL_DELETE,
        )

        try:
            delete_docs_and_remove_files(
                db=self.db,
                doc_ids=upsert_doc_ids,
                full_documents_path=self.doc_save_path(),
                keep_latest_version=True,
            )
        except Exception as e:
            self.logger.error(
                f"Failed to delete upserted files with error {e}",
                code=LogCode.MODEL_DELETE,
            )

        total_chunks = self.db.retriever.retriever.size()
        self.logger.info(
            f"After removing old doc versions total_chunks={total_chunks}",
            code=LogCode.MODEL_DELETE,
        )

        return successfully_indexed_files

    def rlhf_retraining(self, path: str):
        feedback_samples = defaultdict(int)
        self.logger.info(f"Starting RLHF retraining using file: {path}")
        with open(path, "r") as file:
            for line in file:
                feedback = FeedbackLog.model_validate_json(line)
                if not feedback.perform_rlhf_later:
                    continue

                if feedback.event.action == ActionType.upvote:
                    weight = 2  # Extra weighting for explicit upvotes
                    try:
                        self.db.upvote(
                            queries=feedback.event.queries * weight,
                            chunk_ids=feedback.event.chunk_ids * weight,
                        )
                    except Exception as e:
                        self.logger.error(
                            f"Failed to upvote with error {e}", code=LogCode.MODEL_RLHF
                        )
                        continue
                elif feedback.event.action == ActionType.associate:
                    try:
                        self.db.associate(
                            sources=feedback.event.sources,
                            targets=feedback.event.targets,
                        )
                    except Exception as e:
                        self.logger.error(
                            f"Failed to associate with error {e}",
                            code=LogCode.MODEL_RLHF,
                        )
                        continue
                elif feedback.event.action == ActionType.implicit_upvote:
                    try:
                        self.db.upvote(
                            queries=[feedback.event.query],
                            chunk_ids=[feedback.event.chunk_id],
                        )
                    except Exception as e:
                        self.logger.error(
                            f"Failed to implicit upvote with error {e}",
                            code=LogCode.MODEL_RLHF,
                        )
                        continue

                feedback_samples[feedback.event.action] += 1

        sample_counts = " ".join(f"{k}={v}" for k, v in feedback_samples.items())
        self.logger.info(
            "Completed RLHF supervised training. Samples per feedback type: "
            + sample_counts,
            code=LogCode.MODEL_RLHF,
        )

    def supervised_train(self, files: List[FileInfo]):
        self.logger.info("Starting supervised training.", code=LogCode.MODEL_RLHF)

        successfully_trained_files = 0

        for file in files:
            if file.ext() == ".jsonl":
                try:
                    self.rlhf_retraining(file.path)
                    successfully_trained_files += 1
                except Exception as e:
                    msg = f"Failed to train on file {file.path} with error {e}"
                    self.logger.error(msg, code=LogCode.MODEL_RLHF)
                    self.reporter.report_warning(
                        model_id=self.config.model_id,
                        message=msg,
                    )
            else:
                try:
                    supervised_dataset = ndbv2.supervised.CsvSupervised(
                        file.path,
                        query_column=file.options.get("csv_query_column"),
                        id_column=file.options.get("csv_id_column"),
                        id_delimiter=file.options.get("csv_id_delimiter"),
                    )

                    self.db.supervised_train(supervised_dataset)

                    self.logger.info(
                        f"Completed CSV supervised training on {file.path}.",
                        code=LogCode.MODEL_RLHF,
                    )
                    successfully_trained_files += 1
                except Exception as e:
                    msg = f"Failed to train on file {file.path} with error {e}"
                    self.logger.error(msg, code=LogCode.MODEL_RLHF)
                    self.reporter.report_warning(
                        model_id=self.config.model_id,
                        message=msg,
                    )

        self.logger.info("Completed supervised training.", code=LogCode.MODEL_RLHF)

        return successfully_trained_files

    def train(self, **kwargs):
        """
        Train the NeuralDB with unsupervised and supervised data.
        """
        self.logger.info("Training process started.", code=LogCode.MODEL_TRAIN)
        self.reporter.report_status(self.config.model_id, "in_progress")

        s = time.perf_counter()
        unsupervised_files = self.unsupervised_files()
        e = time.perf_counter()
        self.logger.debug(
            f"Listed {len(unsupervised_files)} unsupervised files in {e-s:.4f} seconds"
        )

        s = time.perf_counter()
        supervised_files = self.supervised_files()
        e = time.perf_counter()
        self.logger.debug(
            f"Listed {len(supervised_files)} supervised files in {e-s:.4f} seconds"
        )

        unsup_start = time.time()
        successfully_indexed_files = 0
        if unsupervised_files:
            check_disk(self.db, self.config.model_bazaar_dir, unsupervised_files)
            successfully_indexed_files = self.unsupervised_train(unsupervised_files)
        unsup_end = time.time()

        # Generative supervised training. Assuming that enough disk space is available for supervised training on model
        if self.config.generative_supervision:
            self.logger.info(f"Starting question generation for supervised training.")
            gen_sup_start = time.time()
            documents = self.sources()
            path_prefix = os.path.join(self.llm_response_dir, "generated_questions")
            os.makedirs(path_prefix, exist_ok=True)
            for doc in documents:
                write_at = os.path.join(path_prefix, f"{doc['source_id']}.csv")
                self.generate_supervise_training_data(
                    doc["source_id"],
                    write_at=write_at,
                )
                supervised_files.append(
                    FileInfo(
                        path=write_at,
                        location=FileLocation.nfs,
                        options={
                            "csv_query_column": "text",
                            "csv_id_column": "chunk_id",
                            "csv_id_delimiter": ":",  # random delimiter because there is only one label per query
                        },
                    )
                )
            self.logger.info(
                f"Completed question generation for supervised training in {time.time() - gen_sup_start:.4f} seconds."
            )

        sup_start = time.time()
        successfully_trained_files = 0
        if supervised_files:
            check_disk(self.db, self.config.model_bazaar_dir, supervised_files)
            successfully_trained_files = self.supervised_train(supervised_files)

        if len(unsupervised_files) > 0 or len(supervised_files) > 0:
            if successfully_indexed_files == 0 and successfully_trained_files == 0:
                msg = "The number of documents indexed and trained is 0. Marking training as failed."
                self.logger.error(msg, code=LogCode.MODEL_TRAIN)
                raise ValueError(msg)
        sup_end = time.time()
        train_time = (unsup_end - unsup_start) + (sup_end - sup_start)
        self.logger.debug(f"Total training time: {train_time:.4f} seconds")

        if self.config.data.deletions:
            delete_docs_and_remove_files(
                db=self.db,
                doc_ids=self.config.data.deletions,
                full_documents_path=self.doc_save_path(),
                keep_latest_version=False,
            )
            self.logger.debug(f"Deleted {len(self.config.data.deletions)} docs.")

        self.save()
        self.logger.info("Model saved successfully.", code=LogCode.MODEL_SAVE)

        self.finalize_training(train_time)
        self.logger.info(
            "Training finalized and reported successfully.", code=LogCode.MODEL_TRAIN
        )

    def evaluate(self, **kwargs):
        """
        Evaluate the NeuralDB. Not implemented.
        """
        self.logger.warning(
            "Evaluation method called. Not implemented.", code=LogCode.MODEL_EVAL
        )

    def save(self):
        try:
            # If its on disk it should already be saved
            if not self.on_disk:
                # if we're retraining from a base model we need to save the in memory chunk_store
                if self.config.base_model_id:
                    os.remove(self.db.chunk_store_path(self.ndb_save_path()))
                    self.db.chunk_store.save(
                        self.db.chunk_store_path(self.ndb_save_path())
                    )
                else:
                    self.db.save(self.ndb_save_path())
                    # TODO(david/kartik) Find out why this fails this fails only sometimes
                    # Its not essential to the platform to remove it so we don't raise
                    # the exception but it saves disk usage.
                    try:
                        shutil.rmtree(self.retriever_save_path())
                    except Exception as e:
                        self.logger.warning(
                            f"Could not delete retriever with error {e}. Continuing without deleting temp retriever.",
                            code=LogCode.MODEL_SAVE,
                        )

                self.logger.info(
                    f"Model saved to {self.ndb_save_path()}", code=LogCode.MODEL_SAVE
                )
        except Exception as e:
            self.logger.error(
                f"Failed to save model with error {e}", code=LogCode.MODEL_SAVE
            )
            raise e

    def get_latency(self) -> float:
        self.logger.debug("Measuring latency of the NeuralDBv2 instance.")
        start_time = time.time()

        self.db.search("Checking for latency", top_k=5)

        latency = time.time() - start_time
        self.logger.info(
            f"Latency measured: {latency} seconds.", code=LogCode.MODEL_INFO
        )
        return latency

    def get_size_in_memory(self) -> int:
        def get_size(path):
            if os.path.isfile(path):
                return os.stat(path).st_size
            return get_directory_size(path)

        # TODO(Nicholas): update this calculation for on_disk=True
        size_in_memory = int(
            get_size(self.db.retriever_path(self.ndb_save_path())) * 1.5
            + get_size(self.db.chunk_store_path(self.ndb_save_path()))
        )
        self.logger.info(
            f"Size of the model in memory: {size_in_memory} bytes",
            code=LogCode.MODEL_INFO,
        )
        return size_in_memory

    def finalize_training(self, train_time: int):
        self.reporter.report_complete(
            model_id=self.config.model_id,
            metadata={
                "num_params": str(self.db.retriever.retriever.size()),
                "size": str(get_directory_size(self.ndb_save_path())),
                "size_in_memory": str(self.get_size_in_memory()),
                "thirdai_version": str(thirdai.__version__),
                "training_time": str(train_time),
                "latency": str(self.get_latency()),
            },
        )

    def full_source_path(self, document: str) -> str:
        return os.path.join(self.doc_save_path(), document)

    def sources(self) -> List[Dict[str, str]]:
        docs = self.db.documents()
        return [
            {
                "source": self.full_source_path(doc["document"]),
                "source_id": doc["doc_id"],
                "version": doc["doc_version"],
            }
            for doc in docs
        ]

    def generate_supervise_training_data(
        self,
        doc_id: str,
        write_at: str,
        batch_size: int = 500,
        questions_per_chunk: int = 2,
    ):
        # 1_000_000 is an absurdly high version number. Basically means "latest version".
        chunk_ids = self.db.chunk_store.get_doc_chunks(
            doc_id=doc_id, before_version=1_000_000
        )

        if not chunk_ids:
            raise ValueError("No chunk found for the given doc_id")

        from csv import writer

        from train_job.prompt_resources.supervise_questions import (
            OpenAIResponse,
            system_prompt,
            user_prompt,
        )

        handler = open(
            write_at,
            "w",
        )
        csv_writer = writer(handler)
        csv_writer.writerow(("text", "chunk_id"))

        for i in range(0, len(chunk_ids), batch_size):
            start_time = time.time()

            # Prepare prompts for these batched_chunks
            batched_prompts = [
                {
                    "prompt": user_prompt[self.config.llm_config.provider].format(
                        questions_per_chunk=questions_per_chunk, chunk_text=chunk.text
                    ),
                    "system_prompt": system_prompt[self.config.llm_config.provider],
                    "metadata": {"chunk_id": chunk.chunk_id},
                    "completion_kwargs": {"max_tokens": 1000},
                }
                for chunk in self.db.chunk_store.get_chunks(
                    chunk_ids[i : i + batch_size]
                )
            ]
            if self.config.llm_config.provider != LLMProvider.cohere:
                for prompt in batched_prompts:
                    prompt["completion_kwargs"]["response_format"] = OpenAIResponse

            batched_response = self.llm.run_and_collect_results(
                batched_prompts, parallelize=True
            )

            if self.config.llm_config.provider not in [
                LLMProvider.cohere,
                LLMProvider.mock,
            ]:
                csv_writer.writerows(
                    [
                        (ques, response[1]["chunk_id"])
                        for response in batched_response
                        for ques in response[0].questions
                    ]
                )
            else:
                csv_writer.writerows(
                    [
                        (ques, response[1]["chunk_id"])
                        for response in batched_response
                        for ques in response[0].split("\n")
                        if ques
                        and len(ques.split())
                        >= 4  # Ignore questions of less than 4 words as it is possibly not a valid question.
                    ]
                )
            handler.flush()
            self.logger.info(
                f"Generated questions for {len(batched_prompts)} chunks in {(time.time() - start_time):.4f} seconds"
            )

        handler.close()
