"""New workflow api

Revision ID: 7750a526cecc
Revises: 263fef17a7b6
Create Date: 2024-10-08 10:02:33.774774

"""

from typing import Sequence, Union

import sqlalchemy as sa
from alembic import op
from sqlalchemy.dialects import postgresql

# revision identifiers, used by Alembic.
revision: str = "7750a526cecc"
down_revision: Union[str, None] = "263fef17a7b6"
branch_labels: Union[str, Sequence[str], None] = None
depends_on: Union[str, Sequence[str], None] = None


COMPONENT_TO_KEY = {"search": "retrieval_id", "guardrail": "guardrail_id"}


def upgrade() -> None:
    # ### commands auto generated by Alembic - please adjust! ###
    model_attributes_table = op.create_table(
        "model_attributes",
        sa.Column("model_id", sa.UUID(), nullable=False),
        sa.Column("key", sa.String(), nullable=False),
        sa.Column("value", sa.String(), nullable=True),
        sa.ForeignKeyConstraint(["model_id"], ["models.id"], ondelete="CASCADE"),
        sa.PrimaryKeyConstraint("model_id", "key"),
    )
    op.create_index("model_attribute", "model_attributes", ["model_id"], unique=False)
    model_dependencies_table = op.create_table(
        "model_dependencies",
        sa.Column("model_id", sa.UUID(), nullable=False),
        sa.Column("dependency_id", sa.UUID(), nullable=False),
        sa.ForeignKeyConstraint(["dependency_id"], ["models.id"], ondelete="CASCADE"),
        sa.ForeignKeyConstraint(["model_id"], ["models.id"], ondelete="CASCADE"),
        sa.PrimaryKeyConstraint("model_id", "dependency_id"),
        sa.UniqueConstraint(
            "model_id", "dependency_id", name="unique_model_dependency"
        ),
    )
    op.create_index(
        "dependency_model_index", "model_dependencies", ["dependency_id"], unique=False
    )
    op.create_index(
        "model_dependency_index", "model_dependencies", ["model_id"], unique=False
    )

    conn = op.get_bind()

    metadata = sa.MetaData()
    metadata.reflect(bind=conn)

    model_table = metadata.tables["models"]
    workflow_type_table = metadata.tables["workflow_types"]
    workflow_table = metadata.tables["workflows"]
    workflow_model_table = metadata.tables["workflow_models"]

    rag_type = conn.execute(
        sa.select(workflow_type_table.c.id).where(workflow_type_table.c.name == "rag")
    ).first()

    if rag_type:
        rag_workflows = conn.execute(
            sa.select(
                workflow_table.c.id,
                workflow_table.c.name,
                workflow_table.c.published_date,
                workflow_table.c.type_id,
                workflow_table.c.user_id,
                workflow_table.c.gen_ai_provider,
            ).where(workflow_table.c.type_id == rag_type.id)
        ).all()

        op.bulk_insert(
            model_table,
            [
                {
                    "id": wf.id,
                    "name": wf.name,
                    "train_status": "complete",
                    "deploy_status": "not_started",
                    "type": "enterprise-search",
                    "downloads": 0,
                    "access_level": "private",
                    "default_permission": "read",
                    "published_date": wf.published_date,
                    "user_id": wf.user_id,
                }
                for wf in rag_workflows
            ],
        )

        op.bulk_insert(
            model_attributes_table,
            [
                {
                    "model_id": wf.id,
                    "key": "llm_provider",
                    "value": wf.gen_ai_provider,
                }
                for wf in rag_workflows
            ],
        )

        rag_components = conn.execute(
            sa.select(workflow_model_table)
            .select_from(
                sa.join(
                    workflow_model_table,
                    workflow_table,
                    workflow_model_table.c.workflow_id == workflow_table.c.id,
                )
            )
            .where(workflow_table.c.type_id == rag_type.id)
        ).all()

        op.bulk_insert(
            model_attributes_table,
            [
                {
                    "model_id": comp.workflow_id,
                    "key": COMPONENT_TO_KEY[comp.component],
                    "value": comp.model_id,
                }
                for comp in rag_components
                if comp.component in COMPONENT_TO_KEY
            ],
        )

        op.bulk_insert(
            model_dependencies_table,
            [
                {
                    "model_id": comp.workflow_id,
                    "dependency_id": comp.model_id,
                }
                for comp in rag_components
                if comp.component in COMPONENT_TO_KEY
            ],
        )

    op.drop_index("model_workflow_index", table_name="workflow_models")
    op.drop_index("workflow_model_index", table_name="workflow_models")
    op.drop_table("workflow_models")
    op.drop_table("workflows")
    op.drop_table("workflow_types")
    op.drop_table("model_shards")

    # Enums need to be dropped too: https://github.com/sqlalchemy/alembic/issues/89
    sa.Enum(name="workflowstatus").drop(op.get_bind(), checkfirst=False)
    # ### end Alembic commands ###


def downgrade() -> None:
    # ### commands auto generated by Alembic - please adjust! ###
    op.create_table(
        "workflow_types",
        sa.Column(
            "id",
            sa.UUID(),
            server_default=sa.text("gen_random_uuid()"),
            autoincrement=False,
            nullable=False,
        ),
        sa.Column("name", sa.VARCHAR(length=256), autoincrement=False, nullable=False),
        sa.Column(
            "description", sa.VARCHAR(length=512), autoincrement=False, nullable=True
        ),
        sa.Column(
            "model_requirements",
            postgresql.JSON(astext_type=sa.Text()),
            autoincrement=False,
            nullable=False,
        ),
        sa.PrimaryKeyConstraint("id", name="workflow_types_pkey"),
        sa.UniqueConstraint("name", name="workflow_types_name_key"),
    )

    op.create_table(
        "workflows",
        sa.Column(
            "id",
            sa.UUID(),
            server_default=sa.text("gen_random_uuid()"),
            autoincrement=False,
            nullable=False,
        ),
        sa.Column("name", sa.VARCHAR(length=256), autoincrement=False, nullable=False),
        sa.Column("type_id", sa.UUID(), autoincrement=False, nullable=False),
        sa.Column("user_id", sa.UUID(), autoincrement=False, nullable=False),
        sa.Column(
            "status",
            postgresql.ENUM("inactive", "active", name="workflowstatus"),
            server_default=sa.text("'inactive'::workflowstatus"),
            autoincrement=False,
            nullable=False,
        ),
        sa.Column(
            "published_date", postgresql.TIMESTAMP(), autoincrement=False, nullable=True
        ),
        sa.Column(
            "gen_ai_provider",
            sa.VARCHAR(length=256),
            autoincrement=False,
            nullable=True,
        ),
        sa.ForeignKeyConstraint(
            ["type_id"], ["workflow_types.id"], name="workflows_type_id_fkey"
        ),
        sa.ForeignKeyConstraint(
            ["user_id"], ["users.id"], name="workflows_user_id_fkey", ondelete="CASCADE"
        ),
        sa.PrimaryKeyConstraint("id", name="workflows_pkey"),
        sa.UniqueConstraint("name", "user_id", name="unique_workflow_name_user"),
    )

    op.create_table(
        "workflow_models",
        sa.Column("workflow_id", sa.UUID(), autoincrement=False, nullable=False),
        sa.Column("model_id", sa.UUID(), autoincrement=False, nullable=False),
        sa.Column(
            "component", sa.VARCHAR(length=256), autoincrement=False, nullable=False
        ),
        sa.ForeignKeyConstraint(
            ["model_id"],
            ["models.id"],
            name="workflow_models_model_id_fkey",
            ondelete="CASCADE",
        ),
        sa.ForeignKeyConstraint(
            ["workflow_id"],
            ["workflows.id"],
            name="workflow_models_workflow_id_fkey",
            ondelete="CASCADE",
        ),
        sa.PrimaryKeyConstraint(
            "workflow_id",
            "model_id",
            "component",
            name="unique_workflow_model_component",
        ),
    )
    op.create_index(
        "workflow_model_index", "workflow_models", ["workflow_id"], unique=False
    )
    op.create_index(
        "model_workflow_index", "workflow_models", ["model_id"], unique=False
    )
    op.create_table(
        "model_shards",
        sa.Column("shard_num", sa.INTEGER(), autoincrement=False, nullable=False),
        sa.Column(
            "train_status",
            postgresql.ENUM(
                "not_started",
                "starting",
                "in_progress",
                "stopped",
                "complete",
                "failed",
                name="status",
                create_type=False,
            ),
            server_default=sa.text("'not_started'::status"),
            autoincrement=False,
            nullable=False,
        ),
        sa.Column("model_id", sa.UUID(), autoincrement=False, nullable=False),
        sa.ForeignKeyConstraint(
            ["model_id"],
            ["models.id"],
            name="model_shards_model_id_fkey",
            ondelete="CASCADE",
        ),
        sa.PrimaryKeyConstraint("shard_num", "model_id", name="model_shards_pkey"),
    )

    op.drop_index("model_dependency_index", table_name="model_dependencies")
    op.drop_index("dependency_model_index", table_name="model_dependencies")
    op.drop_table("model_dependencies")
    op.drop_index("model_attribute", table_name="model_attributes")
    op.drop_table("model_attributes")
    # ### end Alembic commands ###
